{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import falkon\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from falkon import LogisticFalkon\n",
    "from falkon.kernels import GaussianKernel\n",
    "from falkon.options import FalkonOptions\n",
    "from falkon.gsc_losses import WeightedCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Standardize dataset\n",
    "    Args:\n",
    "        X (np.ndarray): Original Dataset\n",
    "    Returns:\n",
    "        np.ndarray: Normalized Dataset\n",
    "    \"\"\"    \n",
    "    X_norm = X.copy()\n",
    "    \n",
    "    for j in range(X_norm.shape[1]):\n",
    "        column = X_norm[:, j]\n",
    "\n",
    "        mean = np.mean(column)\n",
    "        std = np.std(column)\n",
    "    \n",
    "        if np.min(column) < 0:\n",
    "            column = (column-mean)*1./ std\n",
    "        elif np.max(column) > 1.0:                                                                                                                                        \n",
    "            column = column *1./ mean\n",
    "    \n",
    "        X_norm[:, j] = column\n",
    "    \n",
    "    return X_norm\n",
    "    \n",
    "\n",
    "def normalize_features(reference, data):\n",
    "    \"\"\"\n",
    "    Normalize features (higgs normalization)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference : np.ndarray\n",
    "        Numpy array reference sample\n",
    "    data : np.ndarray\n",
    "        Numpy array data sample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ref_norm : np.ndarray\n",
    "        Normalized Numpy array reference sample\n",
    "    data_norm : np.ndarray\n",
    "        Normalized Numpy array data sample\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    X_norm = normalize(np.vstack((reference, data)))\n",
    "    \n",
    "    ref_size = reference.shape[0]\n",
    "    \n",
    "    ref_norm = X_norm[:ref_size, :]\n",
    "    data_norm = X_norm[ref_size:, :]\n",
    "    \n",
    "    return ref_norm, data_norm\n",
    "\n",
    "def generate_data(N_REF, N_BKG, N_SIG, SIG_LOC, SIG_STD, normalize=False):\n",
    "    \"\"\"Generate synthetic data for training and testing a machine learning model.\n",
    "\n",
    "    Args:\n",
    "        config_json (dict): A dictionary containing the configuration parameters for generating the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the feature tensor and target tensor.\n",
    "            The feature tensor has shape (N_samples, N_features) and contains the input features for each sample.\n",
    "            The target tensor has shape (N_samples, 2) and contains the target labels and weights for each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # poisson fluctuate the number of events in each sample\n",
    "    N_bkg_p = int(torch.distributions.Poisson(rate=N_BKG).sample())\n",
    "    N_sig_p = int(torch.distributions.Poisson(rate=N_SIG).sample())\n",
    "    \n",
    "    # the reference rate will not have nuisance parameters\n",
    "    feature_ref_dist = torch.distributions.Exponential(rate=1)\n",
    "\n",
    "    # the data rate will have nuisance parameters   \n",
    "    feature_bkg_dist = torch.distributions.Exponential(rate=1)\n",
    "    feature_sig_dist = torch.distributions.Normal(loc=SIG_LOC, scale=SIG_STD)\n",
    "    \n",
    "    # generate the features\n",
    "    feature_ref  = feature_ref_dist.sample((N_REF, 1))\n",
    "    feature_data = torch.cat(\n",
    "        (\n",
    "            feature_bkg_dist.sample((N_bkg_p, 1)),\n",
    "            feature_sig_dist.sample((N_sig_p, 1))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # concatenate the features\n",
    "    feature = torch.cat((feature_ref, feature_data), dim=0)\n",
    "\n",
    "    # generate the target\n",
    "    target_ref  = torch.zeros((N_REF, 1))\n",
    "    target_data = torch.ones((N_bkg_p + N_sig_p, 1))\n",
    "\n",
    "    target = torch.cat((target_ref, target_data), dim=0)\n",
    "    \n",
    "    # if normalize:\n",
    "    #     feature = norm_func(feature)\n",
    " \n",
    "    return feature_ref, target_data, target_ref, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEPModel_1D:\n",
    "    \"\"\"\n",
    "    Generic model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_params, data_params):\n",
    "    # def __init__(self, N_REF, N_BKG, N_SIG, SIG_LOC, SIG_STD):\n",
    "        \"\"\"Create a model for HEP anomaly detection\n",
    "\n",
    "        Args:\n",
    "            reference_path (str): path of directory containing data used as reference data (set of .h5 files)\n",
    "            data_path (str): path of directory containing data used as datasample (set of .h5 files)\n",
    "            output_path (str): directory in which results will be stored (If the directory doesn't exist it will be created)\n",
    "            norm_fun (Optional, Callable): function used to normalize data. It takes the reference and data samples (two numpy.ndarray) and returns two numpy.ndarray representing the normalized reference and data samples (default to Higgs normalization)\n",
    "        \"\"\"\n",
    "        self.model   = None\n",
    "        self.N_REF   = data_params[\"N_REF\"]\n",
    "        self.N_BKG   = data_params[\"N_BKG\"]\n",
    "        self.N_SIG   = data_params[\"N_SIG\"]\n",
    "        self.SIG_LOC = data_params[\"SIG_LOC\"]\n",
    "        self.SIG_STD = data_params[\"SIG_STD\"]\n",
    "        \n",
    "        self.sigma = model_params[\"sigma\"]\n",
    "        self.penalty_list = model_params[\"penalty_list\"]\n",
    "        self.iter_list = model_params[\"iter_list\"]\n",
    "        self.M = model_params[\"M\"]\n",
    "        self.keops = model_params[\"keops_active\"]\n",
    "        self.seed = model_params[\"seed\"]\n",
    "        \n",
    "    @property\n",
    "    def model_seed(self):\n",
    "        if self.model is not None:\n",
    "            return self.model.seed\n",
    "        raise Exception(\"Model is not built yet!\")\n",
    "    \n",
    "    def generate_data(self, N_REF, N_BKG, N_SIG, SIG_LOC, SIG_STD, normalize=False):\n",
    "        \"\"\"Generate synthetic data for training and testing a machine learning model.\n",
    "\n",
    "        Args:\n",
    "            config_json (dict): A dictionary containing the configuration parameters for generating the data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the feature tensor and target tensor.\n",
    "                The feature tensor has shape (N_samples, N_features) and contains the input features for each sample.\n",
    "                The target tensor has shape (N_samples, 2) and contains the target labels and weights for each sample.\n",
    "        \"\"\"\n",
    "        # poisson fluctuate the number of events in each sample\n",
    "        N_bkg_p = int(torch.distributions.Poisson(rate=N_BKG).sample())\n",
    "        N_sig_p = int(torch.distributions.Poisson(rate=N_SIG).sample())\n",
    "        \n",
    "        feature_ref_dist = torch.distributions.Exponential(rate=1)\n",
    "\n",
    "        feature_bkg_dist = torch.distributions.Exponential(rate=1)\n",
    "        feature_sig_dist = torch.distributions.Normal(loc=SIG_LOC, scale=SIG_STD)\n",
    "        \n",
    "        # generate the features\n",
    "        feature_ref  = feature_ref_dist.sample((N_REF, 1))\n",
    "        feature_data = torch.cat(\n",
    "            (\n",
    "                feature_bkg_dist.sample((N_bkg_p, 1)),\n",
    "                feature_sig_dist.sample((N_sig_p, 1))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # concatenate the features\n",
    "        feature = torch.cat((feature_ref, feature_data), dim=0)\n",
    "\n",
    "        # generate the target\n",
    "        target_ref  = torch.zeros((N_REF, 1))\n",
    "        target_data = torch.ones((N_bkg_p + N_sig_p, 1))\n",
    "\n",
    "        target = torch.cat((target_ref, target_data), dim=0)\n",
    "        \n",
    "\n",
    "        return feature, target, feature_ref, feature_data, target_ref, target_data\n",
    "    \n",
    "    def build_model(self):\n",
    "        kernel = GaussianKernel(torch.Tensor([self.sigma]))\n",
    "        cg_tol = 1e-7\n",
    "        keops_active = \"auto\"\n",
    "        use_cpu = False\n",
    "        weight = self.N_BKG / self.N_REF\n",
    "\n",
    "        configuration = {\n",
    "            'kernel' : kernel,\n",
    "            'penalty_list' : self.penalty_list,\n",
    "            'iter_list' : self.iter_list,\n",
    "            'M' : self.M,\n",
    "            'options' : FalkonOptions(cg_tolerance=cg_tol, keops_active=keops_active, use_cpu=use_cpu, debug = False),\n",
    "            'loss' : WeightedCrossEntropyLoss(kernel=kernel, neg_weight=weight),\n",
    "            'seed' : self.seed\n",
    "        }\n",
    "\n",
    "        self.model = LogisticFalkon(**configuration)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def learn_t(self):   \n",
    "            \n",
    "        \"\"\"Method used to compute the t values \n",
    "\n",
    "        Args:\n",
    "            R (int): Size of the reference \\(N_0\\)\n",
    "            B (int): Mean of the Poisson distribution from which the size of the background is sampled\n",
    "            S (int): Mean of the Poisson distribution from which the size of the signal is sampled\n",
    "            features (list[str]): List containing the name of the features used\n",
    "            model_parameters (dict): Dictionary containing the parameters for the model used\n",
    "            sig_type (int): Type of signal (0: no-signal, 1: resonant, 2: non-resonant).\n",
    "            cut (int, optional): Cut MLL. Defaults to None.\n",
    "            normalize (bool, optional): If True data will be normalized before fitting the model. Defaults to False.\n",
    "            seeds (tuple[int, int], optional): A tuple (reference_seed, data_seed) used to generate reference and data sample, if None two random seeds are generated. Defaults to None.\n",
    "            pred_features (list[str], optional): List of features to perform predictions. Defaults to None.\n",
    "\n",
    "        \"\"\"        \n",
    "        random_seed = self.seed if self.seed is not None else np.random.randint(1000)\n",
    "        \n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        print(random_seed)\n",
    "\n",
    "        feature, target, feature_ref, feature_data, target_ref, target_data = self.generate_data(self.N_REF, self.N_BKG, self.N_SIG, self.SIG_LOC, self.SIG_STD)\n",
    "        \n",
    "        # data = feature\n",
    "  \n",
    "        # Create and fit model\n",
    "        weight = self.N_BKG / self.N_REF \n",
    "        self.build_model()\n",
    "\n",
    "        Xtorch = feature\n",
    "        Ytorch = target      \n",
    "\n",
    "        train_time = time.time()\n",
    "        self.fit(Xtorch, Ytorch)\n",
    "        train_time = time.time() - train_time\n",
    "\n",
    "        ref_pred, data_pred = self.predict(feature_ref), self.predict(feature_data)\n",
    "\n",
    "        # Compute Nw and t\n",
    "        # Nw = weight*torch.sum(torch.exp(ref_pred))\n",
    "        diff = weight*torch.sum(1 - torch.exp(ref_pred))\n",
    "        t = 2 * (diff + torch.sum(data_pred).item()).item()\n",
    "\n",
    "        del data_pred, Xtorch, Ytorch\n",
    "        return t, train_time#, ref_seed, data_seed#, ref_pred.numpy().reshape(-1)#ref_pred\n",
    "\n",
    "\n",
    "\n",
    "    def save_result(self, fname, i, t, Nw, train_time, ref_seed, sig_seed):\n",
    "        \"\"\"Function which save the result of learn_t in a file\n",
    "\n",
    "        Args:\n",
    "            fname (str): File name in which the result will be stored (the file will be stored in the output path specified). \n",
    "            If the file already exists, the result will be appended. \n",
    "            i (int): Toy identifier\n",
    "            t (float): value of t obtained\n",
    "            Nw (float): Nw\n",
    "            train_time (float): Time spent in fitting the model\n",
    "            ref_seed (int): seed to reproduce the reference sample\n",
    "            sig_seed (int): seed to reproduce the data sample\n",
    "        \"\"\"        \n",
    "        with open(self.output_path + \"/{}\".format(fname), \"a\") as f:\n",
    "            f.write(\"{},{},{},{},{},{},{}\\n\".format(i, t, Nw, train_time, ref_seed, sig_seed, self.model_seed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HEPModel_1D at 0x7f2f73764f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters = {\n",
    "    'sigma' : 0.3, # kernel lengthscale\n",
    "    'penalty_list' : [1e-7], # list of regularization parameters,\n",
    "    'iter_list' : [1000], #list of number of CG iterations,\n",
    "    'M' : 3000, #number of Nystrom centers,\n",
    "    'keops_active': \"auto\", # optional, if it is used, pyKeOPS is used to speed-up computations\n",
    "    'seed' : None # (int or None), the model seed (used for Nystrom center selection) is manually set.\n",
    " }\n",
    "\n",
    "\n",
    "data_params = {\n",
    "    \"N_REF\"   : 200000,\n",
    "    \"N_BKG\"   : 2000,\n",
    "    \"N_SIG\"   : 0,\n",
    "    \"SIG_LOC\" : 6.4,\n",
    "    \"SIG_STD\" : 0.16,\n",
    "}\n",
    "\n",
    "model = HEPModel_1D(model_parameters, data_params)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 1000\n",
      "19.547821044921875\n"
     ]
    }
   ],
   "source": [
    "t, _ = model.learn_t()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "912\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "853\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "308\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "951\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "14\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "619\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "764\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "702\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "722\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "616\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "570\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "597\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "160\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "872\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "953\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "816\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "596\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "667\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "213\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "299\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "730\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "260\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "10\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "265\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "53\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "537\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "108\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "99\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "641\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "507\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "624\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "444\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "387\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "879\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "123\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "510\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "952\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "170\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "922\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "219\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "449\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "217\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "658\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "454\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n",
      "699\n",
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 5\n"
     ]
    }
   ],
   "source": [
    "toys = 100\n",
    "t_list = []\n",
    "\n",
    "for toy in range(toys):\n",
    "    t, _ = model.learn_t()\n",
    "    t_list.append(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.10251464, 0.10983712, 0.12448207, 0.04393485, 0.0292899 ,\n",
       "        0.05125732, 0.10251464, 0.08054722, 0.05125732, 0.03661237]),\n",
       " array([24.22436142, 25.59001999, 26.95567856, 28.32133713, 29.6869957 ,\n",
       "        31.05265427, 32.41831284, 33.78397141, 35.14962997, 36.51528854,\n",
       "        37.88094711]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(t_list, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEPMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LogFalkonHep import LogFalkonHEPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "    'sigma' : 2.0, # kernel lengthscale\n",
    "    'penalty_list' : [1e-7], # list of regularization parameters,\n",
    "    'iter_list' : [100000], #list of number of CG iterations,\n",
    "    'M' : 3000, #number of Nystrom centers,\n",
    "    'keops_active': \"auto\", # optional, if it is used, pyKeOPS is used to speed-up computations\n",
    "    'seed' : None # (int or None), the model seed (used for Nystrom center selection) is manually set.\n",
    " }\n",
    "\n",
    "data_params = {\n",
    "    \"N_REF\"   : 200000,\n",
    "    \"N_BKG\"   : 2000,\n",
    "    \"N_SIG\"   : 0,\n",
    "    \"SIG_LOC\" : 6.4,\n",
    "    \"SIG_STD\" : 0.16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogFalkonHEPModel(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Falkon/nn_model.py:174\u001b[0m, in \u001b[0;36mHEPModel.learn_t\u001b[0;34m(self, model_parameters, seeds)\u001b[0m\n\u001b[1;32m    171\u001b[0m Ytorch \u001b[38;5;241m=\u001b[39m target   \n\u001b[1;32m    173\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtorch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m train_time\n\u001b[1;32m    177\u001b[0m ref_pred, data_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(feature_ref), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(feature_data)\n",
      "File \u001b[0;32m~/Falkon/nn_model.py:135\u001b[0m, in \u001b[0;36mHEPModel.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/models/logistic_falkon.py:251\u001b[0m, in \u001b[0;36mLogisticFalkon.fit\u001b[0;34m(self, X, Y, Xts, Yts)\u001b[0m\n\u001b[1;32m    248\u001b[0m         knmp_hess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mknmp_hess(X, ny_X, Y, inner_mmv_, precond\u001b[38;5;241m.\u001b[39minvT(sol_a), opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m precond\u001b[38;5;241m.\u001b[39minvAt(precond\u001b[38;5;241m.\u001b[39minvTt(knmp_hess)\u001b[38;5;241m.\u001b[39madd_(sol_a\u001b[38;5;241m.\u001b[39mmul_(penalty_)))\n\u001b[0;32m--> 251\u001b[0m     optim_out \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     beta_it \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m precond\u001b[38;5;241m.\u001b[39minvA(optim_out)\n\u001b[1;32m    254\u001b[0m t_elapsed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_s\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/optim/conjgrad.py:129\u001b[0m, in \u001b[0;36mConjugateGradient.solve\u001b[0;34m(self, X0, B, mmv, max_iter, callback)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TicToc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChol Iter\u001b[39m\u001b[38;5;124m\"\u001b[39m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m     t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 129\u001b[0m     AP \u001b[38;5;241m=\u001b[39m \u001b[43mmmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m Rsold \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39msum(P \u001b[38;5;241m*\u001b[39m AP, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(m_eps))\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# X += P @ diag(alpha)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/models/logistic_falkon.py:247\u001b[0m, in \u001b[0;36mLogisticFalkon.fit.<locals>.mmv\u001b[0;34m(sol, inner_mmv_, penalty_)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmmv\u001b[39m(sol, inner_mmv_\u001b[38;5;241m=\u001b[39minner_mmv, penalty_\u001b[38;5;241m=\u001b[39mpenalty):\n\u001b[0;32m--> 247\u001b[0m     sol_a \u001b[38;5;241m=\u001b[39m \u001b[43mprecond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvA\u001b[49m\u001b[43m(\u001b[49m\u001b[43msol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     knmp_hess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mknmp_hess(X, ny_X, Y, inner_mmv_, precond\u001b[38;5;241m.\u001b[39minvT(sol_a), opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m precond\u001b[38;5;241m.\u001b[39minvAt(precond\u001b[38;5;241m.\u001b[39minvTt(knmp_hess)\u001b[38;5;241m.\u001b[39madd_(sol_a\u001b[38;5;241m.\u001b[39mmul_(penalty_)))\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/preconditioner/pc_utils.py:28\u001b[0m, in \u001b[0;36mcheck_init.<locals>._checker.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_init:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFALKON preconditioner is not initialized. Please run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`init` before any other method on the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreconditioner.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/preconditioner/logistic_preconditioner.py:226\u001b[0m, in \u001b[0;36mLogisticPreconditioner.invA\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Solve the system of equations :math:`Ax = v` for unknown vector :math:`x`.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mMultiple right-hand sides are supported (by simply passing a 2D tensor for `v`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m:func:`falkon.preconditioner.pc_utils.trsm` : the function used to solve the system of equations\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m inplace_set_diag_th(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfC, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdA)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/la_helpers/wrapper.py:124\u001b[0m, in \u001b[0;36mtrsm\u001b[0;34m(v, A, alpha, lower, transpose)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA and v must be on the same device.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m         A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 124\u001b[0m vout \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_trsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(vout)\n",
      "File \u001b[0;32m~/miniconda3/envs/falkon-env/lib/python3.10/site-packages/falkon/la_helpers/cpu_trsm.py:11\u001b[0m, in \u001b[0;36mcpu_trsm\u001b[0;34m(A, v, alpha, lower, transpose)\u001b[0m\n\u001b[1;32m      9\u001b[0m trsm_fn \u001b[38;5;241m=\u001b[39m choose_fn(A\u001b[38;5;241m.\u001b[39mdtype, sclb\u001b[38;5;241m.\u001b[39mdtrsm, sclb\u001b[38;5;241m.\u001b[39mstrsm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRSM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m vF \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(v, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrsm_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous:\n\u001b[1;32m     13\u001b[0m     vF \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(vF, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = model.learn_t(model_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falkon-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
